{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's file path\n",
    "file_path = Path(\"models/model_adam_5.h5\")\n",
    "\n",
    "# Load the model to a new object\n",
    "adam_5 = tf.keras.models.load_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 111, 111, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 394272)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                25233472  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,234,433\n",
      "Trainable params: 25,234,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "adam_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key Loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load env variables    \n",
    "load_dotenv()\n",
    "\n",
    "# Add your OpenAI API key here\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print (f\"OpenAI API Key Loaded: {openai_api_key is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberlykruel/anaconda3/envs/dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer for translation\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# Set source language\n",
    "tokenizer.src_lang = 'en_XX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Language information MBart\n",
    "language_info = [\n",
    "    \"English (en_XX)\", \"Arabic (ar_AR)\", \"Czech (cs_CZ)\", \"German (de_DE)\",\n",
    "    \"Spanish (es_XX)\", \"Estonian (et_EE)\", \"Finnish (fi_FI)\", \"French (fr_XX)\",\n",
    "    \"Gujarati (gu_IN)\", \"Hindi (hi_IN)\", \"Italian (it_IT)\", \"Japanese (ja_XX)\",\n",
    "    \"Kazakh (kk_KZ)\", \"Korean (ko_KR)\", \"Lithuanian (lt_LT)\", \"Latvian (lv_LV)\",\n",
    "    \"Burmese (my_MM)\", \"Nepali (ne_NP)\", \"Dutch (nl_XX)\", \"Romanian (ro_RO)\",\n",
    "    \"Russian (ru_RU)\", \"Sinhala (si_LK)\", \"Turkish (tr_TR)\", \"Vietnamese (vi_VN)\",\n",
    "    \"Chinese (zh_CN)\", \"Afrikaans (af_ZA)\", \"Azerbaijani (az_AZ)\", \"Bengali (bn_IN)\",\n",
    "    \"Persian (fa_IR)\", \"Hebrew (he_IL)\", \"Croatian (hr_HR)\", \"Indonesian (id_ID)\",\n",
    "    \"Georgian (ka_GE)\", \"Khmer (km_KH)\", \"Macedonian (mk_MK)\", \"Malayalam (ml_IN)\",\n",
    "    \"Mongolian (mn_MN)\", \"Marathi (mr_IN)\", \"Polish (pl_PL)\", \"Pashto (ps_AF)\",\n",
    "    \"Portuguese (pt_XX)\", \"Swedish (sv_SE)\", \"Swahili (sw_KE)\", \"Tamil (ta_IN)\",\n",
    "    \"Telugu (te_IN)\", \"Thai (th_TH)\", \"Tagalog (tl_XX)\", \"Ukrainian (uk_UA)\",\n",
    "    \"Urdu (ur_PK)\", \"Xhosa (xh_ZA)\", \"Galician (gl_ES)\", \"Slovene (sl_SI)\"\n",
    "]\n",
    "\n",
    "# Convert the information into a dictionary\n",
    "language_dict = {}\n",
    "for info in language_info:\n",
    "    name, code = info.split(' (')\n",
    "    code = code[:-1]  \n",
    "    language_dict[name] = code\n",
    "\n",
    "# Get the language names for choices in the dropdown\n",
    "languages = list(language_dict.keys())\n",
    "first_language = languages[0]\n",
    "sorted_languages = sorted(languages[1:])\n",
    "sorted_languages.insert(0, first_language)\n",
    "\n",
    "default_language = 'English'\n",
    "\n",
    "# Prediction responses\n",
    "malignant_text = \"Malignant. Please consult a doctor for further evaluation.\"\n",
    "benign_text = \"Benign. Please consult a doctor for further evaluation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get system and human messages for ChatOpenAI - Predictions\n",
    "def get_prediction_messages(prediction_text):\n",
    "    # Create a HumanMessage object\n",
    "    human_message = HumanMessage(content=f'skin lesion that appears {prediction_text}')\n",
    "    \n",
    "    # Get the system message\n",
    "    system_message =  SystemMessage(content='You are a medical professional chatting with a patient. You want to provide helpful information and give a preliminary assessment.')\n",
    "    \n",
    "    # Return the system message\n",
    "    return [system_message, human_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get system and human messages for ChatOpenAI - Help\n",
    "def get_chat_messages(chat_prompt):\n",
    "    # Create a HumanMessage object\n",
    "    human_message = HumanMessage(content=chat_prompt)\n",
    "    \n",
    "    # Get the system message\n",
    "    system_message =  SystemMessage(content='You are a medical professional chatting with a patient. You want to provide helpful information.')\n",
    "    # Return the system message\n",
    "    return [system_message, human_message]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to predict the image\n",
    "def predict_image(language, img):\n",
    "    try:       \n",
    "        try: \n",
    "            # Process the image\n",
    "            img = img.resize((224, 224))\n",
    "            img_array = np.array(img) / 255.0\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "            return 'There was an error processing the image. Please try again.'\n",
    "        \n",
    "        # Get prediction from model\n",
    "        prediction = adam_5.predict(img_array)\n",
    "        text_prediction = 'Malignant' if prediction[0][0] > 0.5 else 'Benign'\n",
    "        \n",
    "        try:\n",
    "            # Get the system and human messages\n",
    "            messages = get_prediction_messages(text_prediction)\n",
    "            \n",
    "            # Get the response from ChatOpenAI\n",
    "            result = llm(messages)\n",
    "            \n",
    "            # Get the text prediction\n",
    "            text_prediction = f'Prediction: {text_prediction} Explanation: {result.content}'\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "            print(f'Prediction: {text_prediction}')\n",
    "            text_prediction = malignant_text if text_prediction == 'Malignant' else benign_text \n",
    "\n",
    "        # Get selected language code\n",
    "        selected_code = language_dict[language]\n",
    "        \n",
    "        # Check if the target and source languages are the same\n",
    "        if selected_code == 'en_XX':          \n",
    "            return text_prediction, gr.update(visible=False), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True)\n",
    "        \n",
    "        try:\n",
    "            # Encode, generate tokens, decode the prediction\n",
    "            encoded_text = tokenizer(text_prediction, return_tensors=\"pt\")\n",
    "            generated_tokens = model.generate(\n",
    "                **encoded_text,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[selected_code]\n",
    "            )\n",
    "            result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            # Return the result\n",
    "            return result[0], gr.update(visible=False), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True)\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "            return f'''There was an error processing the translation. \n",
    "            In English:\n",
    "            {text_prediction}\n",
    "            ''', gr.update(visible=False), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        return 'There was an error processing the request. Please try again.', gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for on submit\n",
    "def on_submit(language, img):\n",
    "    print(f'Language: {language}')\n",
    "    if language is None or len(language) == 0:\n",
    "        language = default_language\n",
    "    if img is None:\n",
    "        return 'No image uploaded. Please try again.', gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n",
    "    return predict_image(language, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for on clear\n",
    "def on_clear():\n",
    "    return gr.update(), gr.update(), gr.update(), gr.update(visible=True), gr.update(value=None, visible=False), gr.update(value = None, visible=False), gr.update(visible=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for on chat\n",
    "def on_chat(language, chat_prompt):\n",
    "    try:\n",
    "        # Get the system and human messages\n",
    "        messages = get_chat_messages(chat_prompt)\n",
    "        # Get the response from ChatOpenAI\n",
    "        result = llm(messages)\n",
    "        # Get the text prediction\n",
    "        chat_response = result.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        return gr.update(value='There was an error processing your question. Please try again.', visible=True), gr.update(visible=False)\n",
    "\n",
    "    # Get selected language code\n",
    "    if language is None or len(language) == 0:\n",
    "        language = default_language\n",
    "    selected_code = language_dict[language]\n",
    "    # Check if the target and source languages are the same\n",
    "    if selected_code == 'en_XX':\n",
    "        return gr.update(value=chat_response, visible=True), gr.update(visible=False)\n",
    "        \n",
    "    try:\n",
    "        # Encode, generate tokens, decode the prediction\n",
    "        encoded_text = tokenizer(chat_response, return_tensors=\"pt\")\n",
    "        generated_tokens = model.generate(\n",
    "            **encoded_text,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[selected_code]\n",
    "        )\n",
    "        result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "        # Return the result\n",
    "        return gr.update(value= result[0], visible=True), gr.update(visible=False)\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        return gr.update(value= f'''There was an error processing the translation. \n",
    "            In English:\n",
    "            {chat_response}\n",
    "            ''', visible=True), gr.update(visible=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 4.4.1, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://4f5c2482c165f43213.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4f5c2482c165f43213.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: []\n",
      "1/1 [==============================] - 0s 109ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 21:15:27.286501: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "/Users/kimberlykruel/anaconda3/envs/dev/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Chinese\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "# Gradio app\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"green\")) as demo:\n",
    "    intro = gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Welcome to Skin Lesion Image Classifier!\n",
    "    Select prediction language and upload image to start.\n",
    "    \"\"\")\n",
    "    language = gr.Dropdown(\n",
    "        label = 'Response Language - Default English',\n",
    "        choices = sorted_languages\n",
    "    )\n",
    "    img = gr.Image(image_mode='RGB', type='pil')\n",
    "    output = gr.Textbox(label='Results', show_copy_button=True)\n",
    "    chat_prompt = gr.Textbox(\n",
    "        label='Do you have a question about the results or skin cancer?', \n",
    "        placeholder='Enter your question here...',\n",
    "        visible=False)\n",
    "    chat_response = gr.Textbox(label='Chat Response', visible=False, show_copy_button=True)\n",
    "    submit_btn = gr.Button('Submit', variant='primary', visible=True)\n",
    "    chat_btn = gr.Button('Submit Question', variant='primary', visible=False)\n",
    "    submit_btn.click(fn=on_submit, inputs=[language, img], outputs=[output, submit_btn, chat_prompt, chat_btn, chat_response])   \n",
    "    chat_btn.click(fn=on_chat, inputs=[language,chat_prompt], outputs=[chat_response, chat_btn])\n",
    "    clear_btn = gr.ClearButton(components=[language, img, output, chat_response], variant='stop')\n",
    "    clear_btn.click(fn=on_clear, outputs=[language, img, output, submit_btn, chat_prompt, chat_response, chat_btn])\n",
    "    \n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
