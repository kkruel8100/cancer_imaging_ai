{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KTSnyder/ASU-VIRT-AI-PT-11-2023-U-LOLC/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Set the model's file path\n",
    "file_path = Path(\"models/model_adam_scaled.h5\")\n",
    "\n",
    "# Load the model to a new object\n",
    "adam_5 = tf.keras.models.load_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">394272</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,233,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m394272\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │    \u001b[38;5;34m25,233,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,234,435</span> (96.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,234,435\u001b[0m (96.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,234,433</span> (96.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,234,433\u001b[0m (96.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model summary\n",
    "adam_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key Loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load env variables    \n",
    "load_dotenv()\n",
    "\n",
    "# Add your OpenAI API key here\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print (f\"OpenAI API Key Loaded: {openai_api_key is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KTSnyder/ASU-VIRT-AI-PT-11-2023-U-LOLC/.conda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create instance\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ChatGPT function using the OpenAI API\n",
    "def chatgpt(input_text):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": input_text}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ChatGPT request: {e}\")\n",
    "        return \"There was an error processing the chat request. Please try again.\"\n",
    "# Placeholder functions (should be defined with actual implementation)\n",
    "def preprocess_image(img):\n",
    "    img = img.resize((224, 224))\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = img_array[np.newaxis, ...]\n",
    "    return img_array\n",
    "\n",
    "def interpret_prediction(prediction):\n",
    "    return \"Prediction interpretation placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer for translation\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "sorted_languages = sorted(tokenizer.lang_code_to_id.keys())\n",
    "selected_code = tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "\n",
    "\n",
    "# Set source language\n",
    "tokenizer.src_lang = 'en_XX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder functions (should be defined elsewhere in your notebook)\n",
    "def preprocess_image(img):\n",
    "    # Preprocess the image before prediction\n",
    "    pass\n",
    "\n",
    "def interpret_prediction(prediction):\n",
    "    # Interpret the model's prediction\n",
    "    return \"Prediction interpretation placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get system and human messages for ChatOpenAI\n",
    "def get_messages(prediction_text):\n",
    "    # Create a HumanMessage object\n",
    "    human_message = HumanMessage(content=f'skin lesion that appears {prediction_text}')\n",
    "    \n",
    "    # Get the system message\n",
    "    system_message =  SystemMessage(content='You are a medical professional chatting with a patient. You want to provide helpful information and give a preliminary assessment.')\n",
    "    \n",
    "    # Return the system message\n",
    "    return [system_message, human_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: None\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KTSnyder/ASU-VIRT-AI-PT-11-2023-U-LOLC/.conda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat prompt received: Where is a dermotologist?\n",
      "Error in ChatGPT request: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ChatGPT response: There was an error processing the chat request. Please try again.\n"
     ]
    }
   ],
   "source": [
    "# Language information MBart\n",
    "language_info = [\n",
    "    \"English (en_XX)\", \"Arabic (ar_AR)\", \"Czech (cs_CZ)\", \"German (de_DE)\",\n",
    "    \"Spanish (es_XX)\", \"Estonian (et_EE)\", \"Finnish (fi_FI)\", \"French (fr_XX)\",\n",
    "    \"Gujarati (gu_IN)\", \"Hindi (hi_IN)\", \"Italian (it_IT)\", \"Japanese (ja_XX)\",\n",
    "    \"Kazakh (kk_KZ)\", \"Korean (ko_KR)\", \"Lithuanian (lt_LT)\", \"Latvian (lv_LV)\",\n",
    "    \"Burmese (my_MM)\", \"Nepali (ne_NP)\", \"Dutch (nl_XX)\", \"Romanian (ro_RO)\",\n",
    "    \"Russian (ru_RU)\", \"Sinhala (si_LK)\", \"Turkish (tr_TR)\", \"Vietnamese (vi_VN)\",\n",
    "    \"Chinese (zh_CN)\", \"Afrikaans (af_ZA)\", \"Azerbaijani (az_AZ)\", \"Bengali (bn_IN)\",\n",
    "    \"Persian (fa_IR)\", \"Hebrew (he_IL)\", \"Croatian (hr_HR)\", \"Indonesian (id_ID)\",\n",
    "    \"Georgian (ka_GE)\", \"Khmer (km_KH)\", \"Macedonian (mk_MK)\", \"Malayalam (ml_IN)\",\n",
    "    \"Mongolian (mn_MN)\", \"Marathi (mr_IN)\", \"Polish (pl_PL)\", \"Pashto (ps_AF)\",\n",
    "    \"Portuguese (pt_XX)\", \"Swedish (sv_SE)\", \"Swahili (sw_KE)\", \"Tamil (ta_IN)\",\n",
    "    \"Telugu (te_IN)\", \"Thai (th_TH)\", \"Tagalog (tl_XX)\", \"Ukrainian (uk_UA)\",\n",
    "    \"Urdu (ur_PK)\", \"Xhosa (xh_ZA)\", \"Galician (gl_ES)\", \"Slovene (sl_SI)\"\n",
    "]\n",
    "\n",
    "# Convert the information into a dictionary\n",
    "language_dict = {}\n",
    "for info in language_info:\n",
    "    name, code = info.split(' (')\n",
    "    code = code[:-1]  \n",
    "    language_dict[name] = code\n",
    "\n",
    "# Get the language names for choices in the dropdown\n",
    "languages = list(language_dict.keys())\n",
    "first_language = languages[0]\n",
    "sorted_languages = sorted(languages[1:])\n",
    "sorted_languages.insert(0, first_language)\n",
    "\n",
    "default_language = 'English'\n",
    "\n",
    "# Prediction responses\n",
    "malignant_text = \"Malignant. Please consult a doctor for further evaluation.\"\n",
    "benign_text = \"Benign. Please consult a doctor for further evaluation.\"\n",
    "\n",
    "def submit(language, img):\n",
    "    print(f'Language: {language}')\n",
    "    if language is None or len(language) == 0:\n",
    "        language = default_language\n",
    "    if img is None:\n",
    "        return 'No image uploaded. Please try again.'\n",
    "    return predict_image(language, img)\n",
    "\n",
    "def predict_image(language, img):\n",
    "    try:       \n",
    "        try: \n",
    "            # Process the image\n",
    "            img = img.resize((224, 224))\n",
    "            img_array = np.array(img) / 255.0\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "            return 'There was an error processing the image. Please try again.'\n",
    "        \n",
    "        # Get prediction from model\n",
    "        prediction = adam_5.predict(img_array)\n",
    "        text_prediction = 'Malignant' if prediction[0][0] > 0.5 else 'Benign'\n",
    "        \n",
    "        try:\n",
    "            # Get the system and human messages\n",
    "            messages = get_messages(text_prediction)\n",
    "            \n",
    "            # Get the response from ChatOpenAI\n",
    "            result = llm(messages)\n",
    "            \n",
    "            # Get the text prediction\n",
    "            text_prediction = result.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "            print(f'Prediction: {text_prediction}')\n",
    "            text_prediction = malignant_text if text_prediction == 'Malignant' else benign_text \n",
    "\n",
    "        # Get selected language code\n",
    "        selected_code = language_dict[language]\n",
    "        \n",
    "        # Check if the target and source languages are the same\n",
    "        if selected_code == 'en_XX':\n",
    "            return text_prediction\n",
    "        \n",
    "        try:\n",
    "            # Encode, generate tokens, decode the prediction\n",
    "            encoded_text = tokenizer(text_prediction, return_tensors=\"pt\")\n",
    "            generated_tokens = model.generate(\n",
    "                **encoded_text,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[selected_code]\n",
    "            )\n",
    "            result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            # Return the result\n",
    "            return result[0]\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "            return f'''There was an error processing the translation. \n",
    "            In English:\n",
    "            {text_prediction}\n",
    "            '''\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        return 'There was an error processing the request. Please try again.'\n",
    "    \n",
    "# Define the chat function to handle user queries\n",
    "def chat(chat_prompt):\n",
    "    print(f\"Chat prompt received: {chat_prompt}\")\n",
    "    if chat_prompt:\n",
    "        chat_response = chatgpt(chat_prompt)\n",
    "        print(f\"ChatGPT response: {chat_response}\")\n",
    "        return chat_response\n",
    "    return \"Please enter a prompt to chat.\"\n",
    "    \n",
    "    \n",
    "# Redefine the Gradio interface to include the chat functionality\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"green\")) as demo:\n",
    "    intro = gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Welcome to Skin Lesion Image Classifier!\n",
    "    Select prediction language and upload image to start.\n",
    "    \"\"\")\n",
    "    language = gr.Dropdown(\n",
    "        label='Change Language - Default English',\n",
    "        choices=sorted_languages\n",
    "    )\n",
    "    img = gr.Image(image_mode='RGB', type='pil')\n",
    "    submit_btn = gr.Button('Submit', variant='primary')\n",
    "    output = gr.Textbox(label='Results')\n",
    "    chat_prompt = gr.Textbox(label='Do you have any questions about the results, skin cancer, where to find a dermotologist, or something else?', placeholder='Enter your question here...')\n",
    "    chat_btn = gr.Button('Submit Question', variant='secondary')\n",
    "    chat_output = gr.Textbox(label='Question Response')\n",
    "    submit_btn.click(fn=submit, inputs=[language, img], outputs=output)\n",
    "    chat_btn.click(fn=chat, inputs=[chat_prompt], outputs=chat_output)\n",
    "    clear_btn = gr.ClearButton(components=[language, img, output, chat_prompt, chat_output], variant='stop')\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
